## 1. 토픽 모델링

- 토픽 (Topic)
    - 텍스트 데이터에서 함께 자주 등장하는 단어들의 반복적인 패턴

- 토픽 모델링 (Topic Modeling)
    - Unlabeled 상태의 대규모 텍스트 데이터를 분석하여 자동으로 토픽을 식별
    - 입력 : 대규모 문서 집합(text collection)의 어휘 분포 정보
    - 출력 : 문서의 주요 토픽 집합 & 구성된 단어들과 비율
    - 역할
        - 코퍼스 내의 어떤 **토픽**들이 포함되어 있는지 식별
        - 각 문서가 어떤 토픽을 어느 정도의 확률로 포함하는지 분석
    - 토픽모델링 알고리즘 : LSI, LDA
    - 툴킷 : Mallet, TMT (Java) & Gensim, Scikit-learn, pyLDAvis (python)

- LDA (Latent Dirichlet Allocation)
    - 가정 :
        - 각 문서는 하나 이상의 토픽으로 구성
        - 문서와 단어 사이에 숨겨진 토픽층(topic layer)이 존재
    - 각 토픽은 해당 토픽 내에서 상대적으로 높은 분포 비율을 보이는 키워드 목록으로 구성
    - 반복적인 학습 과정을 통해 키워드의 가중치가 계산돼 안정화
        
        ![2024-12-13_19-13-55.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/f4d5860d-58b5-4362-bd7e-e645d50c3116/2024-12-13_19-13-55.jpg)
        

## 2. 토픽모델링 사례

- 동아일보 데이터(1946~2014)를 활용
- 북한 관련 키워드가 포함된 기사 16만건 추출
- 토픽 수 10개로 선정 (경험적)
- 결과
    - 10개 토픽의 키워드
        
        ![2024-12-13_19-17-26.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/0c53cd52-3b97-4292-ba90-6295176cec5b/2024-12-13_19-17-26.jpg)
        
    - 연도별 분포
        
        ![2024-12-13_19-18-12.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/6479b243-c1c4-454d-a596-d2c08906011e/2024-12-13_19-18-12.jpg)
        

## 3. Mallet 패키지 실습

- MALLET
    - 통계적 자연어 처리를 위한 자바 기반 패키지로, 토픽 모델링과 정보 추출을 지원
    - Java 기반의 패키지

1. 코퍼스 수집
    - 분석 대상 코퍼스 선정 & 수집 - 적어도 200개 이상의 문서 요구됨
2. 코퍼스 전처리
    - 토큰화 & 형태소 분석
    - 색인어 추출 - 품사 한정 : 명사(NNG, NNP), 한자(SH), 영어(SL)인 형태소만 사용
    - 불용어 (의미를 거의 담고있지 않는 고빈도 단어) 선정 → stoplists로 설정
3. 토픽모델링
    1. mallet으로 import
        
        ```python
        # $1 : text file
        # $2 : mallet file
        bin/mallet import-file --input $1 --output $2 
        --stoplist-file stoplists/kr.txt 
        --keep-sequence TRUE 
        --token-regex '[\p{L}\p{P}]*\p{L}'
        ```
        
    2. 모델링
        
        ```python
        # $1 : mallet file
        # $2 : number of topics
        bin/mallet train-topics --input $1 
        --output-model $1.$2.model
        --num-topics $2
        --output-doc-topics $1.$2.doc-topics
        --output-topic-keys $1.$2.topic-keys
        ```
        
4. 결과 분석
    1. topic-keys 파일
        - 토픽과 토픽에 해당하는 키워드 목록 정보
            
            ![2024-12-14_17-56-57.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/0689e2a8-02bd-4773-9654-ff09ad22b0ae/2024-12-14_17-56-57.jpg)
            
    2. doc-topic 파일
        - 문서의 토픽 비율 정보 → 문서 간의 토픽 분포 및 연도별 관련성 파악 가능
            
            ![2024-12-14_17-57-12.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/8c0e7cf5-7258-4f70-8e01-558cda763564/2024-12-14_17-57-12.jpg)
            
        - 토픽의 전체 분포
            
            ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/903b588b-c67f-46f2-b44b-37cc9bf11e07/image.png)
            
        - 연도별 토픽 분포
            
            ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/1b3055ba-63d0-4cfd-858a-7794ba9a7fea/image.png)
            
        - 신문사별 토픽 분포
            
            ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/3969909d-cad4-4f53-a48a-1e660007a5a4/image.png)
            

## 4. Gensim 패키지 실습

1. 입력 파일
    - 토큰 파일 : 특정 주제의 키워드를 갖고 있는 문서
    - 기본 단위가 문서이기 때문에, 같은 기사 내에 있는 모든 색인어들을 한 줄에 작성
        
        ![2024-12-14_18-13-07.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/3d354b10-365b-437c-b769-effa7f35658d/2024-12-14_18-13-07.jpg)
        

1. LDA 학습
    
    ```python
    def lda_learn(file):
    
    		# 입력 파일 읽기
        with open(file) as fin:
            documents = fin.readlines()
    
    		# 입력 파일을 이중 리스트로 저장
        texts = [[word for word in document.split()] for document in documents]
    
        # 사전 및 코퍼스 생성 (corpora 활용)
        dictionary = corpora.Dictionary(texts)
        corpus = [dictionary.doc2bow(text) for text in texts]
    
        # pickle로 저장
        with open("corpus.pickle", "wb") as fp:
            pickle.dump((corpus, dictionary), fp)
        
        # 일련의 토픽 수에 대해 모델 학습 및 응집성 점수 계산 & 토픽 수의 범위 설정
        start, end = 4, 16 
        coherence_scores = []
    
        for num_topics in range(start, end+1, 2):
    
            # LDA 모델 학습
            lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)
    
            # 토픽 개수별 모델 저장
            lda_model.save(f"lda_model_{num_topics}")
            
            # 토픽별 키워드
            for idx, topic in lda_model.print_topics(-1):
                
                words, weights = parsing(topic)
    
            coherence_model = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')
            coherence_score = coherence_model.get_coherence()
            coherence_scores.append((num_topics, coherence_score))
    
        # 응집성 결과 출력
        print(coherence_scores)
    
        # 최적의 토픽 수 선택
        optimal_num_topics = start + 2 * coherence_scores.index(max(coherence_scores, key=lambda x: x[1]))
        print(f"Optimal number of topics: {optimal_num_topics}")
    ```
    
- 학습된 LDA 모델로부터 토픽별 **키워드**와 그에 대한 **비율** 출력
    
    ```python
    # 0.023*"kr" + 0.015*"학생" + 0.011*"대학" + 0.011*"교육" + 0.008*"학교" + 0.007*"모집" + 0.006*"선발" + 0.006*"지원" + 0.006*"전형" + 0.006*"아이"
    def parsing(input_string):
        words = []
        weights = []
    
        # 문자열을 공백을 기준으로 분리하여 처리
        for term in input_string.split(' + '):
            weight, word = term.split('*')
            weight = float(weight.strip())
            word = word.strip().strip('\"')  # 따옴표 제거
            words.append(word)
            weights.append(weight)
            
        return words, weights
    ```
    

1. 최적 토픽 개수로 토픽 모델링 시각화
    
    ```python
    def lda_load(num_topics):
        
        # 저장된 모델 로드
        lda_model = LdaModel.load(f"lda_model_{num_topics}")
        
        # 코퍼스
        with open("corpus.pickle", "rb") as fp:
            corpus, dictionary = pickle.load(fp)
        
        # 토픽별 키워드
        for idx, topic in lda_model.print_topics(-1):
            words, weights = parsing(topic)
            print("토픽 #%d:"%idx, words)
    
        # 토픽 수 알아내기
        num_topics = len(lda_model.print_topics(-1))
        topic_dist = [0] * num_topics
        print("# of topics = %d" %num_topics)
    
        # 문서별 토픽 분포
        for i, doc in enumerate(corpus):
            
            topic_probs = lda_model.get_document_topics(doc)
            
            # 1등 토픽
            topic_dist[max(topic_probs, key=lambda x: x[1])[0]] += 1
            
        # 토픽별 문서의 수
        print(topic_dist)
        
        # 시각화
        lda_display = gensimvis.prepare(lda_model, corpus, dictionary)
        pyLDAvis.display(lda_display)
        pyLDAvis.save_html(lda_display, f"visualization_{num_topics}.html")
    ```
    
    ![2024-12-15_00-39-25.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/95ce308c-645c-4be4-b7d6-edaa46fba26b/2024-12-15_00-39-25.jpg)
