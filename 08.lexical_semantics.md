## 1. 어휘 의미론

- 단어의 의미
    - 기존 NLP : 단순한 문자의 나열 or 어휘 목록에서의 인덱스
        
        → 단어의 의미 (ex. 감정적 뉘앙스), 관계(동의어, 반의어) 반영 부족
        

- Lexical semantics (어휘 의미론)
    - 단어의 의미를 연구하는 언어학의 한 분야 → 단어의 의미와 그 맥락에서의 사용 방식을 이해
    - Lemma(표제어) : 단어의 기본 형태
    - Word Sense : 단어가 문맥에 따라 가지는 의미 요소
        - 하나의 lemma는 여러 의미를 가질 수 있으며, 이를 다의성(polysemy)이라고 함

### (1) 단어 유사성

- Synonymy (동의어 관계)
    - 단어가 가진 의미(Word Sense)가 다른 단어의 의미와 완전히 같거나 거의 같은 관계
    - 완전한 동의성이 아니라, 대략적이거나 유사한 의미 관계를 기술하기 위해 사용됨
    - 일부 문맥 또는 모든 문맥에서 같은 의미를 가질 수 있음
        - ex. filbert / hazelnut, couch / sofa, big / large, …
        
- 대조의 원칙 (Principle of Contrast)
    - 언어적 형태의 차이는 의미의 차이를 암시함
    - 실제로 모든 문맥에서 완전히 동일한 단어는 드물며, 미묘한 뉘앙스 차이가 있을 수 있음
        - ex. H2O는 과학적 맥락 / water는 일상적 맥락

- Word Similarity (단어 유사성)
    - 동의어는 아니지만, 일부 의미 요소를 공유하는 단어들이 존재
    - ex. car / bicycle: 둘 다 이동 수단이지만, 방식이나 구조에서 차이가 있음
    - 단어 유사성은 더 큰 의미적 작업에서 유용하게 활용됨
        - 두 단어가 얼마나 비슷한지 아는 것은 두 문장이나 구의 의미 유사성을 계산하는 데 도움
        - QA, paraphrasing, summarization 등의 NLP 작업에서 매우 중요한 역할
    - ex. SimLex-999 데이터셋
        - 두 단어 쌍의 유사한 정보를 수치화한 데이터 (사람이 직접 선정)

### (2) 단어 관련성

- Word Relatedness (단어 관련성)
    - 동의어나 유사성의 관계를 넘어서, 다양한 종류의 관계를 포괄
    - ex. scalpel (메스) / surgeon : 유사성은 없지만 관련된 단어

- Semantic Field
    - 특정 의미 영역을 다루는 단어들의 집합
    - 해당 분야의 개념들이 구조적으로 연결된 관계를 나타냄
    - ex.
        - hospitals : surgeon, scalpel, nurse, anaesthetic, hospital
        - restaurants : waiter, menu, plate, food, chef

- Semantic Frame
    - 특정 유형의 이벤트에서 참여자나 관점을 나타내는 단어들의 집합
    - 단어들이 어떤 사건이나 상황을 설명할 때 어떤 역할을 하는지를 설명하는 구조
    - ex.
        - 상업적 거래(Commercial Transaction) 이벤트
            - 이 사건을 나타내기 위해 사용되는 단어들에는 동사(buy, sell, pay)와 명사(buyer)가 있음
            - Semantic Roles을 가짐 : buyer, seller, goods, money
            - 단어 간 상호 관계 → 문장을 paraphrasing하거나 의미를 변형하는 것이 가능
                - The buyer purchased the goods ↔ The seller sold the goods

## 2. 벡터 의미론

- Vector Semantics (벡터 의미론)
    - 단어 의미를 계산적으로 모델링하는 방법
    - 단어의 맥락(Context)과 분포(Distribution)를 기반으로 의미를 이해하려는 접근법
    - 단어의 의미는 맥락에 의해 결정됨 → 비슷한 맥락에서 자주 나타나는 단어들은 의미적 유사로 간주

- 문맥적 단어 분포로부터 단어의 의미 추론
    - ex. tesgüino
        
        > A bottle of tesgüino is on the table.
        Everybody likes tesgüino.
        Tesgüino makes you drunk.
        We make tesgüino out of corn.
        > 
        
        → 맥락적 단서를 바탕으로 “맥주와 유사한 알코올 음료”라는 의미로 유추
        
        ⇒ 두 단어가 비슷한 맥락에서 나타나면, 의미적으로 유사하다는 직관을 사용
        

### (1) 단어 벡터

- 단어 벡터
    - 단어를 벡터 공간에 임베딩(Embedding)하는 방식으로 의미를 나타냄 → 단어 간의 의미적 유사성 측정
    - 벡터는 단어의 맥락 정보를 기반으로 생성
    - 의미가 비슷한 단어는 가까운 위치에 존재 → **의미 관계**를 벡터 공간 내의 위치와 거리로 나타냄
    - Fine-Grained Model
        - 단어 간 **세밀한 유사성**을 계산할 수 있어, 단순한 동의어나 비슷한 맥락에서만 등장하는 단어도 분석
        - ex. “happy”와 “joyful”은 의미적으로 유사하며, 임베딩 공간에서 가까운 위치를 가짐
            
            ![2024-12-11_05-07-37.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/a5255a2b-4bb5-42d4-b147-6ea7472ffdf6/2024-12-11_05-07-37.jpg)
            

### (2) Term-document Matrix

- Term-Document Matrix (TDM)
    - 텍스트 데이터를 수치적으로 표현하기 위한 기본 방법 중 하나
    - 단어(Term)와 문서(Document) 간의 관계 표시
        
        ![2024-12-11_05-17-20.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/fa7aad42-5ccd-4dda-a63d-3aca91037fe4/2024-12-11_05-17-20.jpg)
        

- Term Frequency
    - 각 셀의 값 $tf_{t,d}$
        - 특정 단어 t가 문서 d에서 등장한 **빈도**
    - 단어 벡터 : 각 단어는 모든 문서에서의 빈도를 나타내는 벡터로 표현
        - $t_{battle} = [1,0,7,13]$
    - 문서 벡터 : 각 문서는 단어 빈도를 나타내는 벡터로 표현
        - $d_{AsYouLikeIt} = [1,114,36,20]$
        
        ![2024-12-11_05-20-03.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/052d0757-a1be-4c1b-a8be-fd719b1299c0/2024-12-11_05-20-03.jpg)
        

- 정보 검색(Information Retrieval)
    - 초기 목적: 문서 간의 유사성을 기반으로 비슷한 문서 검색
    - 문서 벡터가 유사하면 문자는 유사성을 보임
    - 단어 벡터에서도 유사성 확인 가능

### (3) Term-Term Matrix

- Term-Term Matrix
    - 문서 전체 대신 더 작은 문맥에서 단어 관계를 분석
    - 단어 간 관계를 강화 → 단어가 자주 같은 문맥에서 등장하면, 이 두 단어의 벡터는 유사하게 나타남
    - 문맥 = 문단 or Window (중심 단어 주변 ±4개의 단어)
    - 각 단어 집합의 크기 : $|V|$
    - 행렬의 크기 : $|V| \times |V|$  (단어 수 x 단어 수)
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/a3073726-a328-4ffe-b492-e0cdb5a50d31/image.png)
        
        ![2024-12-11_05-40-08.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/2e1729fb-1ca9-48b6-86b5-54b197e4645f/2024-12-11_05-40-08.jpg)
        

- 고차원 희소 행렬
    - 단어 집합의 크기에 따라 고차원 희소 행렬이 됨 (ex. 50,000 x 50,000)
    - 대부분의 셀이 0으로 채워지지만, 이는 효율적인 희소 행렬 알고리즘을 활용하여 처리 가능
    - Window Size에 따른 특성 변화
        - 짧은 윈도우 (±1~3)
            - 문법적 관계에 초점 → 단어 간의 구문적 패턴을 학습
            - ex. “eats”의 문맥은 “he”, “apple”처럼 주어와 목적어와 가까운 단어로 제한
        - 긴 윈도우 (±4~10)
            - 의미적 관계에 초점 → 단어 간의 의미적 유사성을 더 잘 포착
            - ex. “apple”은 “fruit”, “sweet”, “pie” 같은 더 넓은 문맥에서 의미를 학습

## 3. 벡터 간 유사성 측정

### (1) 내적

- Dot Product (내적)
    
    ![2024-12-11_05-58-30.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/0b4e4eb5-4096-4438-ba62-6ca5573a7cb4/2024-12-11_05-58-30.jpg)
    
    - 두 벡터가 같은 방향으로 큰 값을 가지면 **높은 값**을 반환
    - 두 벡터가 직교(orthogonal)하면 결과값은 0

- 문제점
    - 내적은 벡터의 길이에 영향을 받음
    - 더 자주 등장하는 단어는 각 차원에서 더 큰 값을 가지며, 결과적으로 벡터 길이가 길어짐
    1. 빈도에 민감함
        - 고빈도 단어는 벡터 크기가 크므로 다른 단어와 비교했을 때 과도하게 높은 유사도를 가짐
        - ex. “the”와 같은 빈도 높은 단어가 거의 모든 단어와 높은 내적 값을 가짐
    2. 의미적 유사도의 왜곡
        - 실제로 관련 없는 단어라도 빈도 차이로 인해 유사도가 높아질 수 있음

### (2) 코사인 유사도

- 코사인 유사도
    - 벡터 길이를 정규화하여 벡터 방향만 고려 (유니 벡터)
        
        ![2024-12-11_06-11-05.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/3c19adbd-7187-4fc5-bfec-a21d86671065/2024-12-11_06-11-05.jpg)
        
    - 벡터의 각도 차이만 고려하게 됨
        
        ![2024-12-11_06-11-38.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/cad42fd5-67db-4f0f-8388-da2e12df7855/2024-12-11_06-11-38.jpg)
        
- 유사도 비교
    
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/105647f3-583e-4530-a207-2a5a9365f275/image.png)
    
    - 결과값 확인
        - +1 : 완전히 동일한 방향 → 높은 유사성
        - 0 : 직교 상태 → 관련 없음
        - -1 : 완전히 반대 방향 → 대조적 관계
    - 텍스트 분석에서 단순 빈도값을 사용하는 경우
        
        → 벡터의 값이 0~1로 한정됨
        
- ex.
    
    ![2024-12-11_06-18-53.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/f1235bd6-8301-4adc-99d1-6333f03501bc/2024-12-11_06-18-53.jpg)
    
- Raw Frequency의 한계
    1. 빈도 왜곡
        - 매우 자주 등장하는 단어(the, a, …)가 높은 빈도를 가지지만, 실제 의미적으로 중요하지 않음
        - 빈도 편향(Frequency Bias)을 초래하여 연관성 측정에 부정확성을 만듦
    2. 구별력 부족
        - 단순 빈도는 단어 간의 미묘한 차이를 잘 반영하지 못함
        - ex. “cherry”와 “pie”의 연관성을 명확히 드러내기 어려움

## 4. 가중치 기법

### (1) TF-IDF

- 단순히 단어 빈도(TF)에 의존하는 대신, 전체 문서 집합에서의 단어의 등장 분포(IDF)를 함께 고려
    
    → 자주 등장하는 흔한 단어에 대한 가중치를 낮추고, 특정 문서에 특화된 단어를 더 중요하게 취급
    

- TF (Term Frequency, 단어 빈도)
    
    ![2024-12-11_16-29-22.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/d9665f60-518a-4b70-9f28-0a4c2c2b1291/2024-12-11_16-29-22.jpg)
    
- IDF (Inverse Document Frequency, 역문서 빈도)
    
    ![2024-12-11_16-29-50.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/38bd1aaf-300c-4dbd-99fe-71565d5c3c94/2024-12-11_16-29-50.jpg)
    
    - 단어가 전체 문서 집합에서 얼마나 희귀한지를 측정하는 지표
    - N : 전체 문서 수
    - $df_t$ : 단어 t가 등장한 문서 수

- 단어의 중요도
    
    ![2024-12-11_16-30-57.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/54a3ea7b-d8c8-4e51-bf4d-862fbea8b6ee/2024-12-11_16-30-57.jpg)
    

- 단어 유사도 측정 : 코사인 유사도 활용
- 문서 유사도 측정 : 평균 활용
    1. 문서 내 모든 단어에 대해 TF-IDF 벡터를 도출
    2. 해당 문서에 포함된 단어 벡터들의 평균(centroid)을 계산하여 **문서 벡터 설정**
        
        ![2024-12-11_17-02-59.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/bb804036-0ed8-4558-adf9-cd9af28a660c/2024-12-11_17-02-59.jpg)
        
    3. 두 문서 벡터 간 코사인 유사도 계산

### (2) PPMI

- PMI(Pointwise Mutual Information)
    - 두 확률적 사건이 독립적일 때의 공동 등장 빈도에 비해 얼마나 더 함께 등장하는지를 측정하는 지표
    - 두 단어가 서로 독립적으로 발생하는 경우 예상되는 공기 확률보다 실제 등장 확률이 높은가 낮은가를 로그 스케일로 나타냄
        
        $\text{PMI}(\text{word}_1, \text{word}_2) = \log_2 \frac{P(\text{word}_1, \text{word}_2)}{P(\text{word}_1) P(\text{word}_2)}$
        

- PMI의 문제?
    - PMI는 이론상 −∞에서 +∞ 사이 값을 가질 수 있는데, 특히 음수 값은 해석상 문제가 발생할 수 있음
    - 빈도나 확률이 매우 낮은 단어에 대해 음수 PMI를 해석하는 것은 통계적으로 불안정하고, 우연이나 잡음의 영향이 클 수 있음
    - 직관적으로도 “연관성이 없다”는 것보다 “연관성이 있다”는 것을 판단하는 것이 더 명확

- PPMI(Positive Pointwise Mutual Information)
    - PMI의 음수 값을 0으로 치환한 측도
    - 단어 간의 연관성을 측정하는 데 있어 안정적이고 직관적인 값을 제공하는 방법
        
        $\text{PPMI}(\text{word}_1, \text{word}_2) = \max\left(\log_2\frac{P(\text{word}_1,\text{word}_2)}{P(\text{word}_1)P(\text{word}_2)},\, 0\right)$
        

- PPMI 행렬 변환
    1. 빈도 → 확률 변환
        - 전체 빈도 중 비율 계산
        - ex. p(w=information,c=data) = 6/19 = 0.32
            
            
            ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/539f3afa-041c-4eaa-8693-a6a680419239/image.png)
            
            ![2024-12-11_17-46-46.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/53a30c94-e89a-4ab3-9a25-8c087cd39d7b/2024-12-11_17-46-46.jpg)
            
    2. 확률 → PPMI 변환
        - $\text{PMI}(\text{word}_1, \text{word}_2) = \log_2 \frac{P(\text{word}_1, \text{word}_2)}{P(\text{word}_1) P(\text{word}_2)}$
        - ex. pmi(information,data) = log2(0.32 / (0.37 * 0.58)) = 0.58
            
            ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/ec7665ab-4e71-459c-ae15-3ea4ed56f8e2/image.png)
            

### (3) Weighting PMI

- PMI의 빈도 편향 문제
    - 드문 단어들의 등장 확률이 매우 낮기 때문에, 두 희귀 단어가 한 번만이라도 함께 나타나면 PMI 값이 인위적으로 커짐
    - 희귀 이벤트는 빈도에 기반한 추정에서 통계적으로 불안정하며, 한 번의 공기만으로도 큰 의미를 부여하기 어려운 경우가 많음

- 해결책
    1. 희귀 단어의 확률 상향 조정하기
    2. Add-One Smoothing (Laplace Smoothing)

1. 희귀 단어의 확률 상향 조정
    - 모든 확률값을 $\alpha$ 제곱한 뒤 다시 정규화하는 과정
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/8cad0f93-21a7-4256-9abe-c3aa350cf971/image.png)
        
    - ex.  $\alpha$ = 0.75 : 0.99 → 0.97 / 0.01 → 0.03

1. Laplace Smoothing
    - 모든 단어의 빈도에 1 또는 2를 더해주는 방식
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/9d90aed9-b9fa-4e46-8609-6b2a3da8a843/image.png)
        
    - 희귀 단어의 빈도를 0에서 벗어나게 하여 확률 추정을 안정화
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/c1227495-f04e-45fc-8c6e-33be410f9da2/image.png)
        
    - 결과 비교
        
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/877d07fe-8095-40ba-a0d5-e58e0e82edf9/image.png)
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/fe0b2a6e-e159-42cd-9320-b717d6aadb7c/image.png)
