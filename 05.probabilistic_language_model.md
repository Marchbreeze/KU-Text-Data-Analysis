## 1. 언어 모델

- 언어 모델
    - 주어진 텍스트의 **확률**을 계산하는 모델
    - 단어 시퀀스의 확률 P(W) : P(w1, w2, w3, …, wn)
    - 이전 단어를 바탕으로 다음 단어가 나올 확률 : P(wn | w1, w2, w3, …, wn-1)

- 확률의 계산
    
    ![2024-10-18_17-21-36.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/5eb360a9-16f8-44d3-8788-cf780ee1902d/2024-10-18_17-21-36.jpg)
    
    → 0 또는 0/0이 될 가능성이 높음
    
    ⇒ 연쇄 규칙(Chain rule) & 독립 가정을 사용해야 함!
    

### (1) 베이즈 정리

- 조건부확률
    
    $P(A|B) = \frac{P(A, B)}{P(B)}$
    
    - P(A, B): A와 B가 동시에 발생할 확률 (결합 확률)
        
        → $P(A, B) = P(A|B)P(B)$
        

- 베이즈 정리
    - **조건부 확률**을 사용하여 **사전 확률**을 **사후 확률**로 갱신하는 방법
    - **새로운 정보**가 주어졌을 때 기존 지식을 업데이트 가능
    - 사건 A가 발생했을 때 사건 B가 발생할 확률
    - 공식:
        
        $P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}$
        
        - P(A|B): B가 주어졌을 때 A가 일어날 확률 (사후 확률, Posterior Probability)
        - P(B|A): A가 주어졌을 때 B가 일어날 확률 (우도, Likelihood)
        - P(A): A가 일어날 확률 (사전 확률, Prior Probability)
        - P(B): B가 일어날 확률 (정규화 상수)

- 연쇄 규칙 (Chain Rule)
    - P(x1,x2,x3,…xn) = P(x1)P(x2|x1)P(x3|x1,x2)…P(xn|x1…xn-1)
    - ex.
        
        P(나는, 어제, 친구와, 영화를, 봤다)
        = P(나는)
        * P(어제 | 나는)
        * P(친구와 | 나는, 어제)
        * P(영화를 | 나는, 어제, 친구와)
        * P(봤다 | 나는, 어제, 친구와, 영화를)
        

### (2) N-gram 모델

- 마르코프 가정
    - 주어진 현재 상태가 **미래 상태**에 대한 정보를 결정하는 데 있어서 **과거 상태에 의존하지 않는다는 가정**
    - **현재 상태**만으로 **미래 상태**를 예측할 수 있고, **과거의 정보는 무시할 수 있다**는 가정

- 조건부 독립
    - 두 사건 **A**와 **B**가 **다른 사건 C**가 주어진 상태에서 **서로 독립적**
    - $P(A \cap B | C) = P(A | C) \cdot P(B | C)$

- N-gram 모델
    - 이전 N-1개의 단어를 바탕으로 다음에 나올 단어를 예측하는 확률 모델
    - N-gram = 길이가 N인 토큰의 열
    
    - Bigram(2-gram) 모델
        - 두 개의 연속된 단어를 고려하여 다음 단어를 예측
        - $𝑃(𝑤_𝑛 |𝑤_{1,𝑛−1})≈𝑃(𝑤_𝑛 |𝑤_{𝑛−1})$
        - $P(w_{1,n}) \approx \prod_{i=1}^{n} P(w_i | w_{i-1})$
            - 전체 문장에서 단어들이 나올 확률 = 이전 단어가 나왔을 때 현재 단어가 나올 확률의 곱
        - ex. P(“봤다” | “영화를”)
        
    - Trigram(3-gram) 모델
        - 세 개의 연속된 단어를 고려하여 다음 단어를 예측
        - $𝑃(𝑤_𝑛 |𝑤_{1,𝑛−1})≈𝑃(𝑤_𝑛 |𝑤_{𝑛−2,𝑛−1})$
        - $P(w_{1,n}) \approx \prod_{i=1}^{n} P(w_i | w_{i-2,i-1})$
            - 전체 문장에서 단어들이 나올 확률 = 이전 두 단어가 나왔을 때 현재 단어가 나올 확률의 곱
        - ex.  P(“봤다” | “친구와”, “영화를”)

- 최우추정 (MLE)
    - N-gram 모델에서 단어의 확률을 추정하는 방식
    1. Bigram
        - $P(w_i | w_{i-1}) = \frac{\text{freq}(w_{i-1}, w_i)}{\text{freq}(w_{i-1})}$
            - 이전 단어 $w_{i-1}$가 주어졌을 때 **다음 단어 $w_i$**가 나올 확률
            - $w_{i-1}, w_{i}$이 함께 나타난 빈도 / $w_{i-1}$가 나타난 빈도
    2. Trigram
        - $P(w_i | w_{i-2}, w_{i-1}) = \frac{\text{freq}(w_{i-2}, w_{i-1}, w_i)}{\text{freq}(w_{i-2}, w_{i-1})}$

## 2. Bigram 언어 모델 활용 과정

> Berkeley Restaurant Project 예시 문장 9222개
> 
> - *can you tell me about any good cantonese restaurants close by*
> - *mid priced thai food is what i’m looking for*
> - *tell me about chez panisse*
> - *can you give me a listing of the kinds of food that are available*
> - *i’m looking for a good place to eat breakfast*
> - *when is caffe venezia open during the day*

1. Raw Bigram Count (2-gram 빈도수 측정)
    
    ![2024-10-18_21-09-20.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/a922c639-9b45-4ce5-a36e-b853a40224d6/2024-10-18_21-09-20.jpg)
    
    - ex. “I want” : 827번 등장

1. Raw Bigram Probabilities (2-gram 확률 계산)
    
    ![2024-10-18_21-10-40.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/f9ec968d-b5ea-4e13-aa69-14edcf42e366/2024-10-18_21-10-40.jpg)
    
    - ex. P(want | I) = freq(I, want) / freq(I) = 827 / 2533 = 0.33

1. 문장 확률 추정
    
    ![2024-10-18_21-11-51.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/e29928f1-9a48-43fc-aeff-a944f438a0b3/2024-10-18_21-11-51.jpg)
    
    문장 전체의 확률을 계산하기 위해 각 단어 쌍의 Bigram 확률을 곱함
    

1. 언어적 지식 포착
    1. World Knowledge
        - 특정 단어들이 함께 등장할 가능성을 나타냄
        - ex. want 다음에는 english보다 to가 나올 확률이 높음
            - P(to|want) = .66
            - P(english|want) = .0011
    2. Syntax
        - 문법적 지식 반영
            - P(want | spend) = 0
            - P(food | to) = 0

## 3. Shannon’s Method

- Shannon’s Method
    - 확률 기반으로 임의의 문장을 생성하는 방법
    - N-gram 모델을 사용하여 확률적으로 단어를 선택하고 문장을 생성
    - 과정:
        1. 첫단어 : <s>(문장의 시작) 이후에 나올 확률이 가장 높은 단어로 **Bigram** 확률을 이용하여 선택
        2. **두 번째 단어**는 **이전 단어**가 주어진 상태에서 가장 가능성이 높은 단어를 선택
        3. </s> 가 나올 때까지 반복
            
            ![2024-10-18_21-40-27.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/441f3624-d939-4980-b8ca-ad2bde031628/2024-10-18_21-40-27.jpg)
            

- 셰익스피어 문체 생성
    1. Unigram : *To him swallowed confess hear both.*
    2. Bigram : *What means, sir. I confess she then all sorts, he is trim, captain.*
    3. Trigram : *Sweet prince, Falstaff shall die. Harry of Monmouth’s grave.*
    4. Quadrtigram : *King Henry: What! I will go seek the traitor Gloucester.*
    
    → N-gram의 길이에 따라 생성되는 문장이 점점 더 자연스럽고 의미 있게 변화
    
- 월스트리트 저널 문체 생성
    1. Unigram : *Months the my and issue of year foreign new exchange’s* 
    2. Bigram : *Last December through the way to preserve the Hudson corporation*
    3. Trigram : *They also point to ninety nine point six billion dollars from two hundred four*
    
    → 
    
    - 월스트리트 저널의 문체가 셰익스피어와는 매우 다름
    - N-gram 모델이 문체를 분석하고 차이를 나타내는 방법을 설명
