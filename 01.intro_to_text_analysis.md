## 0. 텍스트 마이닝

- 비정형 텍스트 데이터로부터 의미있는 정보를 추출하는 데이터마이닝 기술의 일종
- 대부분의 가치있는 정보/지식은 텍스트로 표현 : 문서, 이메일, 보고서, 웹문서 등
- 주로 기계학습, 자연어처리 기술를 기반으로 수행됨
    
    ![2024-10-15_18-43-36.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/f8f35320-636c-496f-b995-192a333b7320/2024-10-15_18-43-36.jpg)
    

## 1. 단어 빈도 분석

- 단어 빈도 (word frequency)
    - 빈도 : 대상어에 대한 ‘관심’의 정도를 반영하는 지표
    - 관심은 긍정 & 부정 모두 포함

### (1) 단어 빈도 표현 방법

1. 워드 클라우드
    - 단어의 중요도나 빈도에 따라 크기나 색상을 다르게 표현하는 시각화 기법
    - 텍스트 데이터에서 가장 중요한 키워드를 시각적으로 강조할 때 많이 사용
    
2. 어휘 빈도 추이
    - 연도별 / 시기별 단어 빈도의 추이를 관찰
    - 관찰 대상 :
        - 유사어 : 비슷한 의미를 가진 단어들이 사용되는 패턴을 비교
        - 대체어 : 특정 단어가 사라지고 대체어가 사용되기 시작하는 현상을 관찰
        - 빈도가 증가/감소하는 단어 : 특정 시기나 사건에 따라 어떤 단어들이 자주 사용되는지 확인
        - 신어와 소멸한 단어 : 새로운 단어가 생겨나고, 반대로 사라지는 단어들을 분석
    - 단위 빈도 차트 :
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/98729cd3-3e77-493e-bd34-779940105cd8/image.png)
        
3. 통계적 키워드 분석
    - 텍스트 군집(시기별, 주제별, 장르별 등)의 키워드들을 파악

### (2) 단위 빈도 분석에 활용 가능한 도구

1. 구글 북스 프로젝트
    - 3000만권의 책을 디지털화
    - 저작권 문제로 인해 책의 전문을 공개할 수 없지만, 텍스트 분석을 통해 많은 연구에 활용
    
2. 구글 엔그램뷰어
    - 구글 북스 프로젝트에서 수집한 800만 권의 책을 바탕으로 단어 사용 빈도를 분석하는 도구
    - N-gram 분석: 단어의 연속적인 등장 패턴을 분석 (1개에서 5개까지의 단어 묶음)
    - 문화 트렌드 분석: 문화적 트렌드나 특정 개념이 시기별로 어떻게 변해왔는지 정량적으로 분석
    
3. 컬처로믹스
    - 대규모 데이터베이스를 기반으로 한 문화 연구 분야
    - 텍스트 분석을 통해 언어적, 문화적 트렌드를 연구
    - 양적 증거 제시
        - 문화적 트렌드를 정량적으로 분석할 수 있게 하며, 다양한 분야에서 객관적 증거를 제시
    - 활용 분야 :
        1. 언어와 문법 연구 : 특정 단어의 사용 빈도를 분석해 언어의 변화나 문법의 변화를 추적
        2. 사전 편찬 : 단어의 변화나 새로운 단어의 등장을 분석하여 사전 편찬에 활용
        3. 검열 탐지 : 특정 주제나 단어가 급격히 줄어든 경우, 검열이 일어났는지 확인할 수 있음
        4. 문화의 변화 : 문화적 변천을 이해
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/d9447b4b-59b8-4b5e-8780-b1f093d97e35/image.png)
        

## 2. 공기어 분석

- 공기어 분석 (co-occurrence)
    - 대상어와 같은 문맥에서 나타난 단어(공기어)를 통해 해당 단어의 의미, 사용 양상을 파악

- 분산 의미론 (distributional semantics)
    - 유사한 문맥에서 나타나는 단어들은 유사한 의미를 가짐
    - 단어의 의미는 주위에 나타나는 단어들의 분포에 의해 결정됨 (분포 가설)

- 단어 벡터 표현 : Sparse & Dense Vector

### (1) Sparse Vector & 공기어 벡터

- Sparse vector 표현
    - Sparse (희소) : 대부분의 값이 0으로 채워진 벡터
    - 차원의 크기가 큼 (20,000 이상)
    - 공기어 벡터를 기반으로 생성됨
    
- 공기어 벡터
    - 주어진 대상어와 같은 문맥에서 함께 나타난 단어(공기어)들로 이루어진 벡터
        - ex. ‘경제’ ↔ 정책, 성장, 위기, 세계, 성장률, 한국, 미국, 시장, …
    - 벡터의 값은 대상어와 공기어 간의 연관도로 표현
        - 연관도는 공기 빈도, 다이스계수, t-score, 상호정보 등으로 계산

### (2) Dense Vector & 단어 임베딩

- Dense vector 표현
    - Dense (밀집) : 대부분의 값이 non-zero → 벡터의 각 차원에 실질적인 값들이 밀집
    - 작은 차원 : 일반적으로 **200~1000차원** 정도로 표현
    - **Skip-gram &** **CBOW :** Dense Vector를 생성하는 대표적인 알고리즘 중 하나인 **Word2Vec** 모델의 학습 방식
        
        
- 단어 임베딩
    - 단어의 의미를 다차원 벡터 공간 상에 “embedding”함으로써 모델링
    - 단어 임베딩을 통해 단어의 의미를 밀집 벡터로 나타낼 수 있음
    - 벡터들은 유사도 계산이 가능
    - 모델링 기법 : word2vec, GloVe, fastText
        
        ![2024-10-15_19-04-24.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/430919f9-6bbb-4b08-a095-071030ed6203/2024-10-15_19-04-24.jpg)
        
    - 유사도 비교
        
        ![2024-10-15_19-05-01.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/6a4ed2b1-ad7b-4345-8a0e-18b703d5702f/2024-10-15_19-05-01.jpg)
        
- 언어 변화 탐지
    - 단어의 공기 정보를 이용해 언어 변화를 추적하고 발견하는 전산적 접근법
    - 단어 임베딩을 이용
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/b0e64241-cacd-4720-add5-a7b8095a968c/image.png)
        

## 3. 문서 분석

### (1) 분류 & 저자 판별

- 분류
    - 문서를 미리 정한 카테고리로 분류하는 작업
    - ex. 문서 분류, 저자 판별, 감성분석

- 저자 판별 (authorship attribution)
    - 익명 혹은 저자 불명의 저작물에 대해 저자를 할당하는 과업
    - 유사도 기반의 저자 판별
        - 문서 → 벡터 표현
        - 저자 판별을 위한 대상 문서와 각 문서들과의 유사도 계산 → 가장 유사한 문서의 저자 선택

### 2. 군집화 & 토픽 모델링

군집화

- 동일 주제의 문서들을 같은 군집으로 묶는 작업

- 토픽 모델링(Topic modeling)
    - 대량의 텍스트를 분석하는 통계 모델
    - 텍스트를 구성하는 토픽을 자동으로 식별 및 추출하고 이에 대한 분포 정보를 요약
    - 활용 :
        - 코퍼스가 어떤 토픽을 포함하는지, 각 문서에 대한 토픽의 분포를 알아볼 수 있음
        - 흥미로운 주제 발견
        - 시간에 따른 토픽의 패턴을 분석
    - ex. 2000~2013 북한 토픽 모델링
        
        ![2024-10-15_19-10-50.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/18aa1969-d7aa-44b3-8ed8-66cc1abea70b/2024-10-15_19-10-50.jpg)
        
        ![2024-10-15_19-11-38.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/00536d40-3bde-4615-9099-3dda57e8fbfc/2024-10-15_19-11-38.jpg)
        

---

## 분석 과정 예시 - 헤세 작품에 대한 온라인 리뷰

### 1. 원시데이터 (XLS)

- 2종의 헤세 작품에 대한 온라인 리뷰 텍스트 (데미안, 수레바퀴 아래서)
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/881daa78-5837-424c-99ae-37041cd52772/image.png)
    

### 2. 텍스트 추출 & 전처리

- 개행문자 형식 통일 (LF/CR → LF) : 본 데이터에서는 불필요
- 라인 앞/끝 공백 문자 제거, 공백 라인 제거
- HTML entity 변환
    - 예) &lt;데미안&gt; → <데미안>
    - 예) &hellip; → …
    - 예) &#10084; → ❤
- 여러 라인의 텍스트를 한 라인으로 만들기
    - 리뷰 간 구분을 위해
    - 라인이 온전한 문장이 아님 (문장 중간에 줄이 바뀌는 경우)
    
    ![2024-10-16_02-32-27.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/62d0e286-666e-458f-95d7-f4d42d738736/2024-10-16_02-32-27.jpg)
    

### 3. 형태소 분석

- KMAT 형태소 분석기
    - 기계학습 기반 확률 모델을 이용한 형태소 분석기
    - 결과 형식 : `원어절/분석결과`
    - ex.
        
        도서관에서      도서관/NNG+에서/JKB
        읽고                읽/VV+고/EM
        너무                너무/MAG
        마음에             마음/NNG+에/JKB
        들어서             들/VV+어서/EM
        

### 4. 단어 빈도 벡터

- 3가지 품사에 대한 단어 빈도 조사
    1. 명사
        - NNG(일반명사), NNP(고유명사), SL(외국어), SH(한자) 및 이들의 결합형
    2. 동사
        - VV(동사), NNG+XSV(동사파생접미사)
        - ex.
            - 생각하다 → 생각/NNG+하/XSV+ㄴ다/EM
    3. 형용사
        - VA(형용사), NNG+XSA(형용사파생접미사)
        - ex.
            - 특별하다 → 특별/NNG+하/XSA+ㄴ/ETM
            - 자유롭다 → 자유/NNG+롭/XSA+게/EM

![명사 상위 30개](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/ba6efa37-6675-45f3-885c-869d2097e11f/2024-10-16_02-36-03.jpg)

명사 상위 30개

![동사 상위 30개](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/9c188d75-915f-467c-a8f6-370674fac833/2024-10-16_02-36-17.jpg)

동사 상위 30개

### 5. t-socre 계산

- 가정
    
    자신의 문서집합에 자주 등장하되, 다른 문서집합에서는 적게 등장하는 단어일수록 해당 문서집합에 특징적인 단어일 것이다.
    

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/e73569e5-7574-40ac-a511-c979b72016e5/image.png)

![2024-10-16_02-40-39.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/d84f55aa-c4fd-4e96-ad0f-e48f1567069d/2024-10-16_02-40-39.jpg)

![2024-10-16_02-40-47.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/ef41708e-597a-4987-820f-e30c066699e3/2024-10-16_02-40-47.jpg)

![2024-10-16_02-40-55.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/83c7aa17-8ac9-4066-8d61-d9aa890bcb2c/2024-10-16_02-40-55.jpg)

### 6. 벡터 유사도 계산

빈도 벡터 간 코사인 유사도 계산

![2024-10-16_02-41-28.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/8dfe0c13-bfaa-4216-9311-3dadfd52065c/2024-10-16_02-41-28.jpg)

명사 유사도

→ 동일 작품 간 유사도 높음

![2024-10-16_02-41-53.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/02d2cfb6-9d9d-4334-8e12-79e8369807ad/2024-10-16_02-41-53.jpg)

동사 유사도

→ 작품 간 구분 없이 높은 유사도를 보임
