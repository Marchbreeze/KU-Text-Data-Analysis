- 모든 자연어처리(NLP) 과정은 텍스트 정규화를 필요로 함
    1. Tokenizing (segmenting) words
    2. Normalizing word formats
    3. Segmenting sentences

# 1. 토큰화 (Tokenization)

- 토큰화
    - 텍스트를 단어로 분할하는 작업
- 단어 토큰화
    - 텍스트에서 word token 식별
    1. segmented language (영어) : 간단
        - ex. “Apple, Banana, Kiwi” → 토큰 = Apple, Banana, Kiwi
    2. non-segmented language (중국어, 일본어) : 복잡

- Type/Token 구분
    1. Word Type : 다른 단어
    2. Word Token : 같은 단어가 여러번 등장해도 다르게 취급
    - TTR (Type-Token Ratio)
        - 타입과 토큰의 비율 (타입/토큰)
        - 1에서 시작해서 텍스트가 커지면 커질수록 작아짐, 완전이 0으로 가진 않음

## (1) Segmented Language 의 경우

### 공백 기반 (Space-based)

- segmented language의 경우, `공백`, `문장 부호`를 기준으로 토큰 분리
- 리눅스 UNIX에서 명령어로 진행
    
    ```python
    $ tr -sc ‘A-Za-z’ ‘\n’ < sh.txt | sort | uniq -c | sort -nr
    ```
    
    1. 전처리 
        
        `tr -sc 'A-Za-z' '\n'`
        
        - tr : 문자열을 변환하거나 삭제
        - -s : (squeeze) 지정한 문자들이 연속해서 나타나면 하나의 문자로 압축
        - -c : (complement) 명시된 문자 집합 외의 다른 모든 문자를 대상
        - -c 'A-Za-z' : 알파벳 대소문자가 아닌 모든 문자를 대상
        - -s '\n' : 연속된 개행 문자(\n)를 하나의 개행 문자로 압축
    2. 텍스트 입력
        
        `< sh.txt`
        
        - 입력 파일 sh.txt에서 텍스트를 읽어옴
    3. 사전 순 정렬
        
        `| sort`
        
        - 전처리된 토큰(단어)들이 사전 순으로 정렬
    4. 중복 단어 제거
        
        `uniq -c`
        
        - 중복된 단어들을 제거하면서 각 단어가 등장한 횟수를 계산
    5. 숫자 기준 정렬
        
        `sort -nr`
        
        - n : 숫자를 기준으로
        - r : 역순(내림차순)으로 정렬
            
            ![2024-10-17_14-13-04.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/8e1ab0ee-bf11-4ac6-8b4e-3c78ec667b8d/2024-10-17_14-13-04.jpg)
            

- 문장 부호 (Punctuation) 이슈
    1. Word-internal punctuation
        
        M.P.H. / Ph.D. / AT&T
        $45.55 / 555,500.50
        dates (01/02/06)
        URLs (https://www.google.com)
        hashtags (#nlproc)
        email addresses (someone@korea.ac.kr)
        
    2. Clitic contractions (축약 형태)
        
        “are” in we’re
        “am” in I’m
        
    3. Multi-token words
        
        New York / San Francisco / Rock ‘n’ roll
        
        → Multiword Expression Dictionary를 필요로 함
        

### 파이썬 기반 (NLTK)

- NLTK (The Python-based Natural Language Toolkit)
    
    ![2024-10-17_14-20-34.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/a282b44c-74ab-421a-93f1-945055ab6468/2024-10-17_14-20-34.jpg)
    
    - 정규표현식(Regex) 활용

### 언어 특성의 문제

- 언어에 대한 이해도에 따른 토큰 구분 문제

1. 프랑스어의 경우
    - *L'ensemble* : 토큰 분리 어떻게 할지
    
2. 독일어의 경우
    - *Lebensversicherungsgesellschaftsangestellter* (생명 보험 회사 종업원)
    - 복합어의 경우 공백 없이 한 단어로 오는데, 어떻게 분리할지
    - 독일어 정보 검색 시스템 (복합어 분리 모듈) → 성능 15% 향상

## (2) Non-Segmented Language 의 경우

- 공백분자가 없는 언어의 경우, 토큰 경계를 어떻게 설정할지

### 단일 문자 (Single-Character)

- 중국어 (한자)
    - 표의문자 (글자 하나에 의미)
    - 형태소 (morpheme) : 단어의 최소 단위 - 평균 2.4개
    - 단어에 대한 경게를 어떻게 나눌지에 대한 표준이 존재하지 않음

- 경계 설정 예시
    
    ![2024-10-17_14-37-31.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/b01f790c-c830-4fee-bbce-3ba2fc5db7a4/2024-10-17_14-37-31.jpg)
    
- 대부분의 중국어 NLP task에서는 단일 문자(한자)로 구분
    - 단어 분리를 억지로 하지 않고 그냥 한자 하나하나 단위로 그냥 자연어 처리
    - 문자 하나하나가 reasonable semantic level 에 있음을 가정

### 토큰 경계 설정의 문제

- 다른 언어 (일본어, 태국어)의 경우, 더 복잡한 분리를 요함

- 일본어 : 다양한 언어가 섞어있음
    - 가타카나, 히라가나, 한자, 알파벳
        
        ![2024-10-17_14-40-26.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/6d3844b2-82f0-48c1-af7c-36b9c7543f6d/2024-10-17_14-40-26.jpg)
        
    - 머신 러닝으로 처리 (neural sequence model)

## (3) Subword Tokenization

- 문자들의 결합되는 그 빈도 정보를 사용

- 3가지 대표 알고리즘
    1. Byte-Pair Encoding (BPE) 
        - 자주 등장하는 문자나 하위 단어 쌍을 병합해 새로운 토큰을 만드는 방식
    2. Unigram language modeling tokenization 
        - 텍스트를 가장 적합한 하위 단어 단위로 분해하는 방식
    3. WordPiece 
        - 하위 단어의 빈도와 확률에 기반하여 병합을 수행하는 방식
    
- 공통적으로 두개의 파트 있음
    1. token learner : 원시 학습 코퍼스로 vocabulary (토큰 집합) 생성
    2. token segmenter : 원시 학습 문장과 vocabulary로 토큰 생성

## (4) Byte-Pair Encoding (BPE)

- 주어진 텍스트에서 자주 나타나는 문자열(바이트) 쌍을 반복적으로 병합하여 점진적으로 더 긴 서브워드 단위로 변환하는 알고리즘
- 텍스트를 서브워드로 분할하면 희귀한 단어를 다루기 쉬워지고, 언어 모델에서 어휘의 크기를 조절할 수 있음
- 언어 모델에서 희귀한 단어를 처리하고 사전을 효율적으로 줄이기 위해 널리 사용

### Token Learner

- 예시 코퍼스
    
    ![2024-10-17_18-27-56.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/6db7f658-bf13-43ea-8628-e37d4b4c5124/2024-10-17_18-27-56.jpg)
    
1. **초기화**
    - 모든 단어를 문자 수준으로 분할하여 각각의 문자를 토큰으로 시작
    - 각 단어의 끝에는 고유한 구분자(_)를 붙여 단어 경계를 유지
    - ex. "hello" → h e l l o _
    
    ![2024-10-17_18-28-04.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/709d16e0-61b9-4907-be3c-84ddc91eee97/2024-10-17_18-28-04.jpg)
    

1. **빈도 기반 쌍 병합**
    - 각 단어에서 연속적으로 나타나는 문자 쌍을 확인하고, 가장 자주 나타나는 문자 쌍을 병합
    
    ![2024-10-17_18-29-05.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/3b8aeb66-cdb0-4e92-9a20-9a877b69c4a4/2024-10-17_18-29-05.jpg)
    

1. 반복
    - 병합 반복 → 짧은 서브워드가 계속해서 병합되어 더 긴 서브워드 생성
    
    ![2024-10-17_18-29-32.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/7401ce27-66a0-4a75-99ce-d4ef7bf3d87d/2024-10-17_18-29-32.jpg)
    
    ![2024-10-17_18-30-11.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/b2207e88-c25c-48d4-b81a-383c74f9beab/2024-10-17_18-30-11.jpg)
    

1. 학습 종료
    - 미리 정의한 병합 횟수에 도달하면 학습을 종료
    - K번 : 경험적인 결정 (10000~50000)

### Token Segmenter

- 학습된 병합 규칙을 테스트 데이터에 적용하여 토큰화하는 과정

- Greedy 병합
    - 빈도를 고려하지 않고, 학습한 순서대로 테스트 데이터에 적용
    - 탐욕적 적용 - 각 병합 규칙이 적용될 수 있는 부분이 있으면 즉시 적용
    - ex. 학습한 첫 번째 병합 규칙이 e와 r을 er로 병합하는 것이라면, 테스트 데이터에서 이 규칙을 적용할 수 있는 모든 곳에 먼저 적용

- BPE 토큰의 특성
    1. 자주 등장하는 전체 단어 포함
        - 빈도가 높은 쌍부터 병합하기 때문에, 빈도가 높은 단어는 완전한 형태로 토큰화될 가능성이 높음
        - 자주 등장하는 단어들은 완전한 형태로 사전에 포함되어 다른 텍스트에서 그대로 사용
    2. 자주 등장하는 서브워드 포함
        - 단어 내부의 일부 자주 등장하는 패턴, 즉 접사나 어근(접미사, 접두사)을 서브워드로 활용
        - ex. 접미사 -er, -est 같은 형태소는 BPE에 의해 자주 병합되므로, 독립적인 서브워드로 존재
    3. 형태소 기반 서브워드
        - 자주 등장하는 서브워드를 병합하면서, 자연스럽게 단어의 형태소와 비슷한 단위가 됨

- BPE의 이점
    1. 어휘 크기의 효율적 관리
        - BPE는 빈도 기반 병합을 사용하므로, 자주 등장하는 단어와 서브워드를 사전에 포함할 수 있어 어휘 크기를 효과적으로 줄임
        - 자주 등장하는 서브워드도 하나의 토큰으로 병합되기 때문에, 모델이 처리해야 할 어휘의 크기를 크게 줄임
    2. OOV(Out-of-Vocabulary) 문제 해결
        - 자주 등장하지 않는 희귀한 단어는 서브워드 단위로 분해될 수 있기 때문에, 새로운 단어도 적절한 서브워드로 처리할 수 있음
        - ex. "playfulness" 처음 등장 시, 이미 학습된 서브워드 play, ful, ness가 있으면 분해 후 처리
    3. 단어의 의미 보존
        - 형태소 수준의 서브워드를 포함하게 되어 단어의 의미를 보존하면서도 더 작은 단위로 분할
        - 희귀한 단어에서도 의미를 파악하고 처리할 수 있도록 도와줌

---

# 2. 정규화 (Normailization)

- 단어 정규화
    - 여러 형태를 가진 단어 또는 토큰을 **하나의 표준 형식**으로 만드는 작업

- 정규화의 필요성 : 정보 검색
    - 색인된 텍스트와 쿼리에서 사용된 단어들이 같은 형식이어야 검색 결과가 제대로 일치할 수 있음

- 동치류(Equivalence Classes)
    - 단어들을 동등한 것으로 취급하기 위해, 암묵적으로 단어에 대해 동치류를 정의
    - ex.
        - 점 삭제 (*U.S.A.*, *USA)*
        - 하이픈 삭제 (*anti-discriminatory, antidiscriminatory)*
        - 악센트 조정 (프랑스어 *résumé* vs. *resume*)
        - 움라우트 조정 (독일어 *Tuebingen* vs. *Tübingen*)
            - 표준 단어에 악센트가 있더라도, 일반적으로 검색 시 입력하지 않음 → 없는 버전으로 정규화

## (1) **표제어 추출 (Lemmatization)**

- 주어진 단어의 다양한 형태를 하나의 기본 형태(표제어, lemma)로 변환하는 과정
- 단어의 굴절형(어미 변화, 복수형 등)을 줄여서 기본 형태로 변환
- 단어의 정확한 사전 표제어(dictionary headword form)를 찾아야 함

- 형태적으로 복잡한 언어를 처리하는 데 필수적
    - ex. 아랍어 : 단어 형태가 매우 다양하게 변형되므로, 이러한 변형을 표제어로 변환하는 과정이 중요

### 표제어 (Lemma)

- 표제어
    - 어근이 같고 주요 품사와 대략적인 의미가 동일한 단어들의 집합
    - ex.
        - cat = cats
        - am, are, is → be
        - car, cars, car’s, cars’ → car

### 1) 형태소 분석 (Morphological Parsing)

- 표제어 추출의 가장 정교한 방법

- 형태소 (Morphemes)
    - 단어를 구성하는 의미를 가진 가장 작은 단위
    - 어간(Stems): 단어의 핵심 의미를 담고 있는 부분
    - 접사(Affixes): 어간에 붙는 부분 (문법적 기능)

- 형태소 분석
    - 단어를 어간과 접사로 분해하여 그 의미와 문법적 구조를 파악하는 과정
    - ex. cats → cat + s

- 형태소가 복잡한 언어에서는 단어의 여러 형태소를 처리하는 것이 매우 중요
    - ex. 터키어 Uygarlastiramadiklarimizdanmissinizcasina
    - 의미 : 우리가 문명화하지 못한 사람들 중에서 당신들이 있는 것처럼 행동하는
    - 형태소 : Uygar + las + tir + ama + dik + lar + imiz + dan + mis + siniz + casina

### 2) 어간 추출 (Stemming)

- 표제어 추출의 간단한 방법

- 접사를 단순하게 잘라내서 어간(Stem)으로 축소하는 방법
- 언어에 의존적으로 작동
    - ex. automate(s), automatic, automation ⇒ automat

- Porter’s Algorithm
    - 영어에서 가장 많이 사용되는 어간 추출 알고리즘
    - 어휘 사전이 필요하지 않음
    - 굴절형 접미사(ex. 복수형, 시제)와 파생형 접미사(ex. 명사화, 형용사화)를 모두 처리 가능
    - 결과가 실제 어간이라고 보장되지는 않지만, 정보 검색(IR)에서는 이 보장이 크게 중요하지 않음
    - 단어를 처리하기 위해 일련의 재작성 규칙을 순차적으로 적용
        - –*sses* → *ss*
        - –*ies* → *i*
        - –*ational* → *ate*
        - –*ization* → *ize*
        - –*ize* → ε (null)
        - –*ing* → ε (null) if stem contains vowel
    - 규칙이 단어의 measure (음절 길이)에 민감
        - -EMENT → → ε (null) if measure > 1
            - *replacement* → *replac*
            - *cement* → *cement*

# 3. 문장 분리 (Segmentation)

## (1) 이진 분류기 (binary classifier)

- 구두점(., ?, !)으로 문장을 구분하면 정확한 문장 분리에 실패할 가능성이 큼
    - !, ?의 경우 상대적으로 명확함
        - 대부분의 경우 문장의 마무리로 사용됨
    - 마침표의 경우 모호함
        - 영어에서 마침표의 경우 다양한 용도를 가짐 (Mr. , 12.34 , and …)
        - 중국어의 경우 마침표가 없음

- 이진 분류기 (binary classifier)
    - 마침표가 문장의 끝인지 아닌지를 판단하는 모델
    - 마침표를 만났을 때,문맥을 분석하여 이 마침표가 문장의 끝(EOS)인지 아닌지(NotEOS) 결정
    - 접근법
        - 규칙 기반 접근: 사람이 직접 작성한 규칙을 기반으로 판단하는 방식
        - 정규 표현식 기반: 정규 표현식의 일련의 규칙들을 사용하여 문장의 끝을 판단하는 방식
        - 머신러닝 기반: 학습된 모델이 데이터를 기반으로 마침표가 문장의 끝인지 아닌지 예측하는 방식

- NLTK에서 문장 분류기 진행 가능

## (2) 품사 태깅 (POS Tagging)

- 텍스트 내의 각 단어에 대해 품사(POS, part-of-speech) 또는 어휘적 클래스를 할당하는 과정
- 문장에서 단어가 어떤 품사인지(명사, 동사, 형용사 등)를 표시하는 작업
    
    ![2024-10-17_23-58-33.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/9e707f98-e031-4669-b59d-59c9316ea742/2024-10-17_23-58-33.jpg)
    

- 품사 태깅은 실용적 작업의 첫번쨰 단계로 필수적
- 사례:
    1. 음성 합성 (Speech Synthesis)
        - 단어의 품사에 따라 발음이 달라질 수 있음
        - 강세 변화 예시 :
            - INsult (명사) vs inSULT (동사)
            - OBject (명사) vs obJECT (동사)
            - CONtent (명사) vs conTENT (형용사)
    2. 단어 의미 중의성 해소
        - 문장을 분석하고, 단어의 의미가 여러 가지일 때 적절한 의미를 선택하는 데 도움
    3. 정보 추출
        - 이름, 시간, 날짜 등 특정 정보를 추출할 때 사용 (인명, 지명 등의 명사 식별)
    4. 언어 연구
        - 언어 구조와 문법적 패턴 분석하는데 도움

- POS Tag Set의 종류
    1. Penn Treebank POS Tag Set
        - 영어 코퍼스를 주로 대상으로 한 태깅 시스템
        - 세부적인 품사 정보를 제공하는 36개의 주요 POS 태그와 12개의 구두점 태그로 구성
            
            ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/31318434-fb52-4fe7-8b41-36039da8dfb6/image.png)
            
    2. Universal POS Tag Set
        - 간단하고 보편적인 태그 세트로, 다양한 언어에 적용할 수 있도록 고안
        - 17개의 품사로 구성되어 있으며, 언어 간의 공통된 문법 범주를 반영
            
            ![2024-10-18_00-20-24.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/117bbd2a-9cef-42e4-8196-826225c38b8c/2024-10-18_00-20-24.jpg)
            

- 품사 태깅이 어려운 이유
    - 영어 단어 대부분은 명확한 품사를 가짐 (약 88.5%)
    - 그러나, 자주 쓰이는 단어들은 품사가 모호
        - 40% 이상의 토큰은 여러 품사를 가질 수 있어, 문맥에 따라 의미가 달라짐

- 시퀀스 분류 (Sequence Classification)
    - 연속된 관찰값(단어)에 대해 각 단어에 품사 태그를 할당하는 문제
    - 확률적 관점
        - 모든 가능 태그 시퀀스를 고려, 주어진 문장에 대해 가장 확률이 높은 태그 시퀀스를 선택하는 방식
        
- 확률적 모델
    - 단어 시퀀스 (w1…wn)에 대해 가능한 모든 태그 시퀀스 (t1…tn) 중에서, P(t1…tn | w1…wn) 값이 가장 높은 태그 시퀀스를 찾는 과정
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/17735588-ab64-466f-91f5-b99ec4e43db6/image.png)
        
        - $\hat{t}_1^n$ : 추정한 가장 좋은 태그 시퀀스
        - $argmax$ : 모든 가능한 태그 시퀀스 중에서, 확률이 가장 높은 태그 시퀀스를 선택
    - 태그 시퀀스의 가능한 조합이 매우 많기 때문에, 효율적인 방법이 필요

- 베이지안 분류의 직관 (Intuition of Bayesian Classification)
    - 베이즈 정리 사용
    - $P(t1^n | w1^n)$ 를 쉽게 계산할 수 있는 형태로 변환
    - 2-gram Hidden Markov Model
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/8155e0ab-747e-411d-8dc8-1e0e05d1afd3/image.png)
        
        - 이전 하나의 태그만을 고려하여 현재 태그를 예측하는 방식
